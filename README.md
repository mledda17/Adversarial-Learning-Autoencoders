# Repo - Adversarial Learning Attacks on Autoencoder-based models for Nonlinear Dynamical Systems
Marco Ledda, *Student Member, IEEE* 

Alessandro Giua, *Fellow, IEEE*  

Mauro Franceschelli, *Senior Member, IEEE*


## Abstract
The use of learning-based methods in sys-
tems and control is gaining significant interest from the
research community. These methods are flexible and pow-
erful when input/output data is abundant. Despite their
potential, the reliability of these estimation methods can
raise concerns, especially in adversarial settings. Unlike
in the fields of computer vision and image recognition,
which have widely explored adversarial vulnerabilities, ad-
versarial learning in the context of control systems re-
mains an open research topic, largely unexplored. This
paper presents novel adversarial attack techniques for
autoencoder-based models used for system identification.
We propose two scenarios of attack in which we generate
a control input designed to maximize the output error of
the autoencoder, thereby degrading system performance.
Through numerical experiments on benchmark systems,
we provide a counterexample demonstrating the impact
of the proposed attacks. The findings highlight the impor-
tance of developing formal guarantees to enhance the relia-
bility and security of learning-based control systems. Also,
the methods can be used to certify unreliability of models
of the identified systems for certain input sequences.
